{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enes\\anaconda3\\envs\\gputorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", model=\"akdeniz27/bert-base-turkish-cased-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraÅŸ, Etiket: B-LOC\n",
      "Token: elbis, Etiket: B-LOC\n",
      "Token: ##tan, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##Ä±nar, Etiket: B-LOC\n",
      "Token: ##baÅŸÄ±, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##Ä±nar, Etiket: B-LOC\n",
      "Token: ##baÅŸÄ±, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "kahramanmaraÅŸ elbistan pÄ±narbaÅŸÄ± pÄ±narbaÅŸÄ±\n"
     ]
    }
   ],
   "source": [
    "text = \"kahramanmaraÅŸ elbistan pÄ±narbaÅŸÄ± mahallesi pÄ±narbaÅŸÄ± caddesi\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "location = \"\"  \n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    \n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "    if 'LOC' in label:  \n",
    "        if not token.startswith('##'):\n",
    "                if location:  \n",
    "                    location += \" \"  \n",
    "                location += token\n",
    "        else:\n",
    "                location += token.lstrip('##')  \n",
    "\n",
    "\n",
    "print(\"\\nLocation:\")\n",
    "print(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraÅŸ, Etiket: B-LOC\n",
      "Token: elbis, Etiket: B-LOC\n",
      "Token: ##tan, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##Ä±nar, Etiket: B-LOC\n",
      "Token: ##baÅŸÄ±, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##Ä±nar, Etiket: B-LOC\n",
      "Token: ##baÅŸÄ±, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "kahramanmaraÅŸ elbistan pÄ±narbaÅŸÄ± mahallesi pÄ±narbaÅŸÄ± caddesi\n"
     ]
    }
   ],
   "source": [
    "text = \"kahramanmaraÅŸ elbistan pÄ±narbaÅŸÄ± mahallesi pÄ±narbaÅŸÄ± caddesi\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "\n",
    "location = \"\"\n",
    "address_keywords = [\"mahallesi\", \"caddesi\", \"sokak\", \"bulvarÄ±\", \"kÃ¶yÃ¼\", \"yolu\", \"mevkii\"]\n",
    "\n",
    "loc_tokens = []\n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "\n",
    "    if \"LOC\" in label:  \n",
    "        if token.startswith(\"##\"):\n",
    "            loc_tokens[-1] += token.lstrip(\"##\")  \n",
    "        else:\n",
    "            loc_tokens.append(token)\n",
    "\n",
    "words = text.split()  \n",
    "for i, word in enumerate(words):\n",
    "    if any(loc in word for loc in loc_tokens):\n",
    "        location += word + \" \" \n",
    "    if any(keyword in word.lower() for keyword in address_keywords):\n",
    "            location += word + \" \" \n",
    "\n",
    "print(\"\\nLocation:\")\n",
    "print(location.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_ment_link</th>\n",
       "      <th>content_wo_punct</th>\n",
       "      <th>content_wo_removed_english</th>\n",
       "      <th>content_wo_normalize</th>\n",
       "      <th>content_wo_tokenize</th>\n",
       "      <th>content_wo_stop</th>\n",
       "      <th>content_wo_lemmatized</th>\n",
       "      <th>content_no_rare_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Orhan Åevik</td>\n",
       "      <td>orhan_sevik</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>@haluklevent\\n @oguzhanugur\\n  hepinizden alla...</td>\n",
       "      <td>\\n \\n  hepinizden allah razÄ± olsun iyiki varsÄ±...</td>\n",
       "      <td>hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±...</td>\n",
       "      <td>hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±z deprem</td>\n",
       "      <td>hepinizden allah razÄ± olsun iyi ki varsÄ±nÄ±z de...</td>\n",
       "      <td>['hepinizden', 'allah', 'razÄ±', 'olsun', 'iyi'...</td>\n",
       "      <td>['hepinizden', 'razÄ±', 'varsÄ±nÄ±z', 'deprem']</td>\n",
       "      <td>['hep', 'razÄ±', 'var', 'deprem']</td>\n",
       "      <td>['hep', 'razÄ±', 'var', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>nursena</td>\n",
       "      <td>nurssxx_</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem</td>\n",
       "      <td>allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem</td>\n",
       "      <td>allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver deprem</td>\n",
       "      <td>allahÄ±m nolur onlara dayanma gÃ¼cÃ¼ ver deprem</td>\n",
       "      <td>allahÄ±m ne olur onlara dayanma gÃ¼cÃ¼ ver deprem</td>\n",
       "      <td>['allahÄ±m', 'ne', 'olur', 'onlara', 'dayanma',...</td>\n",
       "      <td>['onlara', 'dayanma', 'gÃ¼cÃ¼', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Elif ÅEKER</td>\n",
       "      <td>55ellllllif</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>replying to \\n@haluklevent\\n @danlabilic\\n and...</td>\n",
       "      <td>replying to \\n\\n \\n and \\n_harun\\nadÄ±yamanda a...</td>\n",
       "      <td>replying to      and  harun adÄ±yamanda ali taÅŸ...</td>\n",
       "      <td>harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hic...</td>\n",
       "      <td>harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hicr...</td>\n",
       "      <td>['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...</td>\n",
       "      <td>['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...</td>\n",
       "      <td>['haru', 'adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle',...</td>\n",
       "      <td>['adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle', 'sokak'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Yusuf</td>\n",
       "      <td>yusufaltuns</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n@showtv\\n #deprem</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n\\n #deprem</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun    deprem</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun deprem</td>\n",
       "      <td>tuÄŸba sodekoÄŸlu kovulsun deprem</td>\n",
       "      <td>['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuÄŸba', 'sodekoÄŸlu', 'kov', 'deprem']</td>\n",
       "      <td>['deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>ğÌˆğ³ğ ğ®Ìˆğ« ğ‘ğšğ§</td>\n",
       "      <td>dryghtn2</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...</td>\n",
       "      <td>arkadaÅŸÄ±mÄ±za ulaÅŸamÄ±yoruz kahramanmaraÅŸ elbist...</td>\n",
       "      <td>['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...</td>\n",
       "      <td>['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>2024-11-17 16:28:31</td>\n",
       "      <td>KEREMCGRC</td>\n",
       "      <td>keremcgrci</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>yardÄ±m aÄŸÄ±Â´na destek ol! \\n\\ntwitter'da paylaÅŸ...</td>\n",
       "      <td>yardÄ±m aÄŸÄ±Â´na destek ol! \\n\\ntwitter'da paylaÅŸ...</td>\n",
       "      <td>yardÄ±m aÄŸÄ±Â´na destek ol   twitterda paylaÅŸÄ±lan...</td>\n",
       "      <td>yardÄ±m aÄŸÄ±Â´na destek ol twitterda paylaÅŸÄ±lan a...</td>\n",
       "      <td>yardÄ±m aÄŸÄ±na destek ol twitterda paylaÅŸÄ±lan ad...</td>\n",
       "      <td>['yardÄ±m', 'aÄŸÄ±na', 'destek', 'ol', 'twitterda...</td>\n",
       "      <td>['yardÄ±m', 'aÄŸÄ±na', 'destek', 'ol', 'twitterda...</td>\n",
       "      <td>['yar', 'aÄŸÄ±n', 'destek', 'ol', 'twitter', 'pa...</td>\n",
       "      <td>['yardÄ±m', 'destek', 'ol', 'paylaÅŸ', 'adres', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>2024-11-17 16:28:31</td>\n",
       "      <td>aMMo</td>\n",
       "      <td>tatavaypmadvmet</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>teyiÌ‡tliÌ‡ lÃ¼tfen yardim\\nemlak bank evleri 1. ...</td>\n",
       "      <td>teyiÌ‡tliÌ‡ lÃ¼tfen yardim\\nemlak bank evleri 1. ...</td>\n",
       "      <td>teyiÌ‡tliÌ‡ lÃ¼tfen yardim emlak bank evleri  kÄ±s...</td>\n",
       "      <td>teyiÌ‡tliÌ‡ lÃ¼tfen yardim emlak evleri kÄ±sÄ±m blo...</td>\n",
       "      <td>teyiÌ‡tliÌ‡ lÃ¼tfen yardÄ±m emlak evleri kÄ±sÄ±m blo...</td>\n",
       "      <td>['teyiÌ‡tliÌ‡', 'lÃ¼tfen', 'yardÄ±m', 'emlak', 'ev...</td>\n",
       "      <td>['teyiÌ‡tliÌ‡', 'yardÄ±m', 'emlak', 'evleri', 'kÄ±...</td>\n",
       "      <td>['teyiÌ‡tliÌ‡', 'yar', 'emlak', 'ev', 'kÄ±sÄ±m', '...</td>\n",
       "      <td>['teyiÌ‡tliÌ‡', 'yardÄ±m', 'ev', 'blok', 'hatay',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>2024-11-17 16:28:31</td>\n",
       "      <td>Nâ€™Y</td>\n",
       "      <td>nyonyedi</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>05324707903 babalatv\\n\\nwhastapp hatti tÃ¼m bÃ¶l...</td>\n",
       "      <td>05324707903 babalatv\\n\\nwhastapp hatti tÃ¼m bÃ¶l...</td>\n",
       "      <td>babalatv  whastapp hatti tÃ¼m bÃ¶lgelerdeki yar...</td>\n",
       "      <td>babalatv whastapp hatti tÃ¼m bÃ¶lgelerdeki yardi...</td>\n",
       "      <td>babalatv Whatsapp hattÄ± tÃ¼m bÃ¶lgelerdeki yardÄ±...</td>\n",
       "      <td>['babalatv', 'Whatsapp', 'hattÄ±', 'tÃ¼m', 'bÃ¶lg...</td>\n",
       "      <td>['babalatv', 'Whatsapp', 'hattÄ±', 'bÃ¶lgelerdek...</td>\n",
       "      <td>['babalatv', 'whatsapp', 'hatt', 'bÃ¶lge', 'yar...</td>\n",
       "      <td>['babalatv', 'bÃ¶lge', 'yardÄ±m', 'babalatv', 'o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>2024-11-17 16:28:42</td>\n",
       "      <td>Casuel</td>\n",
       "      <td>allin39949323</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...</td>\n",
       "      <td>iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...</td>\n",
       "      <td>iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...</td>\n",
       "      <td>iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...</td>\n",
       "      <td>Ä±ÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...</td>\n",
       "      <td>['Ä±ÅŸÄ±k', 'olmadÄ±ÄŸÄ±', 'iÃ§in', 'Ã§alÄ±ÅŸmalar', 'du...</td>\n",
       "      <td>['Ä±ÅŸÄ±k', 'Ã§alÄ±ÅŸmalar', 'durmak', 'zorunda', 'k...</td>\n",
       "      <td>['Ä±ÅŸÄ±k', 'Ã§alÄ±ÅŸ', 'dur', 'zor', 'kal', 'acilen...</td>\n",
       "      <td>['Ä±ÅŸÄ±k', 'Ã§alÄ±ÅŸ', 'dur', 'zor', 'kal', 'acilen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>2024-11-17 16:28:42</td>\n",
       "      <td>Mekatronik1907</td>\n",
       "      <td>mekatronik1907</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>tuÄŸba-uÄŸur-nil kÄ±lÄ±nÃ§\\n\\ngeneral ÅŸÃ¼krÃ¼ kanatlÄ±...</td>\n",
       "      <td>tuÄŸba-uÄŸur-nil kÄ±lÄ±nÃ§\\n\\ngeneral ÅŸÃ¼krÃ¼ kanatlÄ±...</td>\n",
       "      <td>tuÄŸbauÄŸurnil kÄ±lÄ±nÃ§  general ÅŸÃ¼krÃ¼ kanatlÄ± yav...</td>\n",
       "      <td>tuÄŸbauÄŸurnil kÄ±lÄ±nÃ§ ÅŸÃ¼krÃ¼ kanatlÄ± yavuz selim ...</td>\n",
       "      <td>tuÄŸbaugÃ¼rnil kÄ±lÄ±nÃ§ ÅŸÃ¼krÃ¼ kanatlÄ± yavuz selim ...</td>\n",
       "      <td>['tuÄŸbaugÃ¼rnil', 'kÄ±lÄ±nÃ§', 'ÅŸÃ¼krÃ¼', 'kanatlÄ±',...</td>\n",
       "      <td>['tuÄŸbaugÃ¼rnil', 'kÄ±lÄ±nÃ§', 'ÅŸÃ¼krÃ¼', 'kanatlÄ±',...</td>\n",
       "      <td>['tuÄŸbaugÃ¼rnil', 'kÄ±lÄ±nÃ§', 'ÅŸÃ¼kr', 'kanat', 'y...</td>\n",
       "      <td>['ÅŸÃ¼kr', 'kanat', 'antakyahatay', 'acil', 'yar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2102 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp        username           handle   tweet_date  \\\n",
       "0     2024-11-03 19:38:25     Orhan Åevik      orhan_sevik  Feb 8, 2023   \n",
       "1     2024-11-03 19:38:25         nursena         nurssxx_  Feb 8, 2023   \n",
       "2     2024-11-03 19:38:25      Elif ÅEKER      55ellllllif  Feb 8, 2023   \n",
       "3     2024-11-03 19:38:25           Yusuf      yusufaltuns  Feb 8, 2023   \n",
       "4     2024-11-03 19:38:25     ğÌˆğ³ğ ğ®Ìˆğ« ğ‘ğšğ§         dryghtn2  Feb 8, 2023   \n",
       "...                   ...             ...              ...          ...   \n",
       "2097  2024-11-17 16:28:31       KEREMCGRC       keremcgrci  Feb 8, 2023   \n",
       "2098  2024-11-17 16:28:31            aMMo  tatavaypmadvmet  Feb 8, 2023   \n",
       "2099  2024-11-17 16:28:31             Nâ€™Y         nyonyedi  Feb 8, 2023   \n",
       "2100  2024-11-17 16:28:42          Casuel    allin39949323  Feb 8, 2023   \n",
       "2101  2024-11-17 16:28:42  Mekatronik1907   mekatronik1907  Feb 8, 2023   \n",
       "\n",
       "                                                content  \\\n",
       "0     @haluklevent\\n @oguzhanugur\\n  hepinizden alla...   \n",
       "1     allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem   \n",
       "2     replying to \\n@haluklevent\\n @danlabilic\\n and...   \n",
       "3          tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n@showtv\\n #deprem   \n",
       "4     arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...   \n",
       "...                                                 ...   \n",
       "2097  yardÄ±m aÄŸÄ±Â´na destek ol! \\n\\ntwitter'da paylaÅŸ...   \n",
       "2098  teyiÌ‡tliÌ‡ lÃ¼tfen yardim\\nemlak bank evleri 1. ...   \n",
       "2099  05324707903 babalatv\\n\\nwhastapp hatti tÃ¼m bÃ¶l...   \n",
       "2100  iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...   \n",
       "2101  tuÄŸba-uÄŸur-nil kÄ±lÄ±nÃ§\\n\\ngeneral ÅŸÃ¼krÃ¼ kanatlÄ±...   \n",
       "\n",
       "                                      content_ment_link  \\\n",
       "0     \\n \\n  hepinizden allah razÄ± olsun iyiki varsÄ±...   \n",
       "1     allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem   \n",
       "2     replying to \\n\\n \\n and \\n_harun\\nadÄ±yamanda a...   \n",
       "3                 tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n\\n #deprem   \n",
       "4     arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...   \n",
       "...                                                 ...   \n",
       "2097  yardÄ±m aÄŸÄ±Â´na destek ol! \\n\\ntwitter'da paylaÅŸ...   \n",
       "2098  teyiÌ‡tliÌ‡ lÃ¼tfen yardim\\nemlak bank evleri 1. ...   \n",
       "2099  05324707903 babalatv\\n\\nwhastapp hatti tÃ¼m bÃ¶l...   \n",
       "2100  iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...   \n",
       "2101  tuÄŸba-uÄŸur-nil kÄ±lÄ±nÃ§\\n\\ngeneral ÅŸÃ¼krÃ¼ kanatlÄ±...   \n",
       "\n",
       "                                       content_wo_punct  \\\n",
       "0          hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±...   \n",
       "1      allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver deprem   \n",
       "2     replying to      and  harun adÄ±yamanda ali taÅŸ...   \n",
       "3                    tuÄŸba sÃ¶dekoÄŸlu kovulsun    deprem   \n",
       "4     arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...   \n",
       "...                                                 ...   \n",
       "2097  yardÄ±m aÄŸÄ±Â´na destek ol   twitterda paylaÅŸÄ±lan...   \n",
       "2098  teyiÌ‡tliÌ‡ lÃ¼tfen yardim emlak bank evleri  kÄ±s...   \n",
       "2099   babalatv  whastapp hatti tÃ¼m bÃ¶lgelerdeki yar...   \n",
       "2100  iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...   \n",
       "2101  tuÄŸbauÄŸurnil kÄ±lÄ±nÃ§  general ÅŸÃ¼krÃ¼ kanatlÄ± yav...   \n",
       "\n",
       "                             content_wo_removed_english  \\\n",
       "0     hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±z deprem   \n",
       "1          allahÄ±m nolur onlara dayanma gÃ¼cÃ¼ ver deprem   \n",
       "2      harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hic...   \n",
       "3                       tuÄŸba sÃ¶dekoÄŸlu kovulsun deprem   \n",
       "4     arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...   \n",
       "...                                                 ...   \n",
       "2097  yardÄ±m aÄŸÄ±Â´na destek ol twitterda paylaÅŸÄ±lan a...   \n",
       "2098  teyiÌ‡tliÌ‡ lÃ¼tfen yardim emlak evleri kÄ±sÄ±m blo...   \n",
       "2099  babalatv whastapp hatti tÃ¼m bÃ¶lgelerdeki yardi...   \n",
       "2100  iÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...   \n",
       "2101  tuÄŸbauÄŸurnil kÄ±lÄ±nÃ§ ÅŸÃ¼krÃ¼ kanatlÄ± yavuz selim ...   \n",
       "\n",
       "                                   content_wo_normalize  \\\n",
       "0     hepinizden allah razÄ± olsun iyi ki varsÄ±nÄ±z de...   \n",
       "1        allahÄ±m ne olur onlara dayanma gÃ¼cÃ¼ ver deprem   \n",
       "2     harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hicr...   \n",
       "3                       tuÄŸba sodekoÄŸlu kovulsun deprem   \n",
       "4     arkadaÅŸÄ±mÄ±za ulaÅŸamÄ±yoruz kahramanmaraÅŸ elbist...   \n",
       "...                                                 ...   \n",
       "2097  yardÄ±m aÄŸÄ±na destek ol twitterda paylaÅŸÄ±lan ad...   \n",
       "2098  teyiÌ‡tliÌ‡ lÃ¼tfen yardÄ±m emlak evleri kÄ±sÄ±m blo...   \n",
       "2099  babalatv Whatsapp hattÄ± tÃ¼m bÃ¶lgelerdeki yardÄ±...   \n",
       "2100  Ä±ÅŸÄ±k olmadÄ±ÄŸÄ± iÃ§in Ã§alÄ±ÅŸmalar durmak zorunda k...   \n",
       "2101  tuÄŸbaugÃ¼rnil kÄ±lÄ±nÃ§ ÅŸÃ¼krÃ¼ kanatlÄ± yavuz selim ...   \n",
       "\n",
       "                                    content_wo_tokenize  \\\n",
       "0     ['hepinizden', 'allah', 'razÄ±', 'olsun', 'iyi'...   \n",
       "1     ['allahÄ±m', 'ne', 'olur', 'onlara', 'dayanma',...   \n",
       "2     ['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...   \n",
       "3          ['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']   \n",
       "4     ['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...   \n",
       "...                                                 ...   \n",
       "2097  ['yardÄ±m', 'aÄŸÄ±na', 'destek', 'ol', 'twitterda...   \n",
       "2098  ['teyiÌ‡tliÌ‡', 'lÃ¼tfen', 'yardÄ±m', 'emlak', 'ev...   \n",
       "2099  ['babalatv', 'Whatsapp', 'hattÄ±', 'tÃ¼m', 'bÃ¶lg...   \n",
       "2100  ['Ä±ÅŸÄ±k', 'olmadÄ±ÄŸÄ±', 'iÃ§in', 'Ã§alÄ±ÅŸmalar', 'du...   \n",
       "2101  ['tuÄŸbaugÃ¼rnil', 'kÄ±lÄ±nÃ§', 'ÅŸÃ¼krÃ¼', 'kanatlÄ±',...   \n",
       "\n",
       "                                        content_wo_stop  \\\n",
       "0          ['hepinizden', 'razÄ±', 'varsÄ±nÄ±z', 'deprem']   \n",
       "1        ['onlara', 'dayanma', 'gÃ¼cÃ¼', 'ver', 'deprem']   \n",
       "2     ['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...   \n",
       "3          ['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']   \n",
       "4     ['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...   \n",
       "...                                                 ...   \n",
       "2097  ['yardÄ±m', 'aÄŸÄ±na', 'destek', 'ol', 'twitterda...   \n",
       "2098  ['teyiÌ‡tliÌ‡', 'yardÄ±m', 'emlak', 'evleri', 'kÄ±...   \n",
       "2099  ['babalatv', 'Whatsapp', 'hattÄ±', 'bÃ¶lgelerdek...   \n",
       "2100  ['Ä±ÅŸÄ±k', 'Ã§alÄ±ÅŸmalar', 'durmak', 'zorunda', 'k...   \n",
       "2101  ['tuÄŸbaugÃ¼rnil', 'kÄ±lÄ±nÃ§', 'ÅŸÃ¼krÃ¼', 'kanatlÄ±',...   \n",
       "\n",
       "                                  content_wo_lemmatized  \\\n",
       "0                      ['hep', 'razÄ±', 'var', 'deprem']   \n",
       "1                ['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']   \n",
       "2     ['haru', 'adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle',...   \n",
       "3               ['tuÄŸba', 'sodekoÄŸlu', 'kov', 'deprem']   \n",
       "4     ['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...   \n",
       "...                                                 ...   \n",
       "2097  ['yar', 'aÄŸÄ±n', 'destek', 'ol', 'twitter', 'pa...   \n",
       "2098  ['teyiÌ‡tliÌ‡', 'yar', 'emlak', 'ev', 'kÄ±sÄ±m', '...   \n",
       "2099  ['babalatv', 'whatsapp', 'hatt', 'bÃ¶lge', 'yar...   \n",
       "2100  ['Ä±ÅŸÄ±k', 'Ã§alÄ±ÅŸ', 'dur', 'zor', 'kal', 'acilen...   \n",
       "2101  ['tuÄŸbaugÃ¼rnil', 'kÄ±lÄ±nÃ§', 'ÅŸÃ¼kr', 'kanat', 'y...   \n",
       "\n",
       "                                  content_no_rare_words  \n",
       "0                      ['hep', 'razÄ±', 'var', 'deprem']  \n",
       "1                ['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']  \n",
       "2     ['adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle', 'sokak'...  \n",
       "3                                            ['deprem']  \n",
       "4     ['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...  \n",
       "...                                                 ...  \n",
       "2097  ['yardÄ±m', 'destek', 'ol', 'paylaÅŸ', 'adres', ...  \n",
       "2098  ['teyiÌ‡tliÌ‡', 'yardÄ±m', 'ev', 'blok', 'hatay',...  \n",
       "2099  ['babalatv', 'bÃ¶lge', 'yardÄ±m', 'babalatv', 'o...  \n",
       "2100  ['Ä±ÅŸÄ±k', 'Ã§alÄ±ÅŸ', 'dur', 'zor', 'kal', 'acilen...  \n",
       "2101  ['ÅŸÃ¼kr', 'kanat', 'antakyahatay', 'acil', 'yar...  \n",
       "\n",
       "[2102 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_keywords = [\"mahallesi\", \"caddesi\", \"sokak\", \"bulvarÄ±\", \"kÃ¶yÃ¼\", \"yolu\", \"mevkii\", \"mah.\", \"cad.\", \"sok.\", \"bul.\", \"kÃ¶y.\", \"yol.\", \"mek.\",\"apartman\",\"apt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_from_text(text):\n",
    "    if not text.strip():\n",
    "        return None \n",
    "    result = pipe(text) \n",
    "    location = \"\" \n",
    "    \n",
    "    for entity in result:\n",
    "        token = entity['word']\n",
    "        label = entity['entity']\n",
    "\n",
    "        if 'LOC' in label:  \n",
    "            if not token.startswith('##'):  \n",
    "                if location:  \n",
    "                    location += \" \" \n",
    "                location += token\n",
    "            else:\n",
    "                location += token.lstrip('##')  \n",
    "    words = text.split()  \n",
    "    for i, word in enumerate(words):\n",
    "        if any(loc in word for loc in loc_tokens):\n",
    "            location += word + \" \" \n",
    "        if any(keyword in word.lower() for keyword in address_keywords):\n",
    "            location += word + \" \" \n",
    "\n",
    "    return location.strip() if location else None\n",
    "df['location'] = df['content_wo_normalize'].apply(get_location_from_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latitude & longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adres: hatay antakya cebrail mahallesi\n",
      "Enlem: 36.2063404, Boylam: 36.1565635\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "# Adres olarak tam metni belirtiyoruz\n",
    "address = \"hatay antakya cebrail mahallesi\"\n",
    "\n",
    "# Adresi enlem ve boylama Ã§evir\n",
    "location = geolocator.geocode(address)\n",
    "\n",
    "if location:\n",
    "    print(f\"Adres: {address}\")\n",
    "    print(f\"Enlem: {location.latitude}, Boylam: {location.longitude}\")\n",
    "else:\n",
    "    print(f\"Adres bulunamadÄ±: {address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "def get_coordinates(address):\n",
    "    if not address or pd.isna(address):\n",
    "        return None, None\n",
    "    try:\n",
    "        location = geolocator.geocode(address, timeout=10)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['latitude', 'longitude']] = df['location'].apply(\n",
    "    lambda x: pd.Series(get_coordinates(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_with_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "m = folium.Map(location=[41.0369, 28.9850], zoom_start=6)\n",
    "\n",
    "for _, row in df_valid.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['location'] \n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"harita.html\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
