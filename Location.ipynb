{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enes\\anaconda3\\envs\\gputorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", model=\"akdeniz27/bert-base-turkish-cased-ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraş, Etiket: B-LOC\n",
      "Token: elbis, Etiket: B-LOC\n",
      "Token: ##tan, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##ınar, Etiket: B-LOC\n",
      "Token: ##başı, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##ınar, Etiket: B-LOC\n",
      "Token: ##başı, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "kahramanmaraş elbistan pınarbaşı pınarbaşı\n"
     ]
    }
   ],
   "source": [
    "text = \"kahramanmaraş elbistan pınarbaşı mahallesi pınarbaşı caddesi\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "location = \"\"  \n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    \n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "    if 'LOC' in label:  \n",
    "        if not token.startswith('##'):\n",
    "                if location:  \n",
    "                    location += \" \"  \n",
    "                location += token\n",
    "        else:\n",
    "                location += token.lstrip('##')  \n",
    "\n",
    "\n",
    "print(\"\\nLocation:\")\n",
    "print(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraş, Etiket: B-LOC\n",
      "Token: elbis, Etiket: B-LOC\n",
      "Token: ##tan, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##ınar, Etiket: B-LOC\n",
      "Token: ##başı, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##ınar, Etiket: B-LOC\n",
      "Token: ##başı, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "kahramanmaraş elbistan pınarbaşı mahallesi pınarbaşı caddesi\n"
     ]
    }
   ],
   "source": [
    "text = \"kahramanmaraş elbistan pınarbaşı mahallesi pınarbaşı caddesi\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "\n",
    "location = \"\"\n",
    "address_keywords = [\"mahallesi\", \"caddesi\", \"sokak\", \"bulvarı\", \"köyü\", \"yolu\", \"mevkii\"]\n",
    "\n",
    "loc_tokens = []\n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "\n",
    "    if \"LOC\" in label:  \n",
    "        if token.startswith(\"##\"):\n",
    "            loc_tokens[-1] += token.lstrip(\"##\")  \n",
    "        else:\n",
    "            loc_tokens.append(token)\n",
    "\n",
    "words = text.split()  \n",
    "for i, word in enumerate(words):\n",
    "    if any(loc in word for loc in loc_tokens):\n",
    "        location += word + \" \" \n",
    "    if any(keyword in word.lower() for keyword in address_keywords):\n",
    "            location += word + \" \" \n",
    "\n",
    "print(\"\\nLocation:\")\n",
    "print(location.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_ment_link</th>\n",
       "      <th>content_wo_punct</th>\n",
       "      <th>content_wo_removed_english</th>\n",
       "      <th>content_wo_normalize</th>\n",
       "      <th>content_wo_tokenize</th>\n",
       "      <th>content_wo_stop</th>\n",
       "      <th>content_wo_lemmatized</th>\n",
       "      <th>content_no_rare_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Orhan Şevik</td>\n",
       "      <td>orhan_sevik</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>@haluklevent\\n @oguzhanugur\\n  hepinizden alla...</td>\n",
       "      <td>\\n \\n  hepinizden allah razı olsun iyiki varsı...</td>\n",
       "      <td>hepinizden allah razı olsun iyiki varsını...</td>\n",
       "      <td>hepinizden allah razı olsun iyiki varsınız deprem</td>\n",
       "      <td>hepinizden allah razı olsun iyi ki varsınız de...</td>\n",
       "      <td>['hepinizden', 'allah', 'razı', 'olsun', 'iyi'...</td>\n",
       "      <td>['hepinizden', 'razı', 'varsınız', 'deprem']</td>\n",
       "      <td>['hep', 'razı', 'var', 'deprem']</td>\n",
       "      <td>['hep', 'razı', 'var', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>nursena</td>\n",
       "      <td>nurssxx_</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>allahım nolur sen onlara dayanma gücü ver #deprem</td>\n",
       "      <td>allahım nolur sen onlara dayanma gücü ver #deprem</td>\n",
       "      <td>allahım nolur sen onlara dayanma gücü ver deprem</td>\n",
       "      <td>allahım nolur onlara dayanma gücü ver deprem</td>\n",
       "      <td>allahım ne olur onlara dayanma gücü ver deprem</td>\n",
       "      <td>['allahım', 'ne', 'olur', 'onlara', 'dayanma',...</td>\n",
       "      <td>['onlara', 'dayanma', 'gücü', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'güc', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'güc', 'ver', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Elif ŞEKER</td>\n",
       "      <td>55ellllllif</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>replying to \\n@haluklevent\\n @danlabilic\\n and...</td>\n",
       "      <td>replying to \\n\\n \\n and \\n_harun\\nadıyamanda a...</td>\n",
       "      <td>replying to      and  harun adıyamanda ali taş...</td>\n",
       "      <td>harun adıyamanda ali taşı mahallesi sokak hic...</td>\n",
       "      <td>harun adıyamanda ali taşı mahallesi sokak hicr...</td>\n",
       "      <td>['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...</td>\n",
       "      <td>['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...</td>\n",
       "      <td>['haru', 'adıyaman', 'ali', 'taşı', 'mahalle',...</td>\n",
       "      <td>['adıyaman', 'ali', 'taşı', 'mahalle', 'sokak'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Yusuf</td>\n",
       "      <td>yusufaltuns</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>tuğba södekoğlu kovulsun \\n@showtv\\n #deprem</td>\n",
       "      <td>tuğba södekoğlu kovulsun \\n\\n #deprem</td>\n",
       "      <td>tuğba södekoğlu kovulsun    deprem</td>\n",
       "      <td>tuğba södekoğlu kovulsun deprem</td>\n",
       "      <td>tuğba sodekoğlu kovulsun deprem</td>\n",
       "      <td>['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuğba', 'sodekoğlu', 'kov', 'deprem']</td>\n",
       "      <td>['deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>𝐎̈𝐳𝐠𝐮̈𝐫 𝐑𝐚𝐧</td>\n",
       "      <td>dryghtn2</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...</td>\n",
       "      <td>arkadaşımıza ulaşamıyoruz kahramanmaraş elbist...</td>\n",
       "      <td>['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...</td>\n",
       "      <td>['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>2024-11-17 16:28:31</td>\n",
       "      <td>KEREMCGRC</td>\n",
       "      <td>keremcgrci</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>yardım ağı´na destek ol! \\n\\ntwitter'da paylaş...</td>\n",
       "      <td>yardım ağı´na destek ol! \\n\\ntwitter'da paylaş...</td>\n",
       "      <td>yardım ağı´na destek ol   twitterda paylaşılan...</td>\n",
       "      <td>yardım ağı´na destek ol twitterda paylaşılan a...</td>\n",
       "      <td>yardım ağına destek ol twitterda paylaşılan ad...</td>\n",
       "      <td>['yardım', 'ağına', 'destek', 'ol', 'twitterda...</td>\n",
       "      <td>['yardım', 'ağına', 'destek', 'ol', 'twitterda...</td>\n",
       "      <td>['yar', 'ağın', 'destek', 'ol', 'twitter', 'pa...</td>\n",
       "      <td>['yardım', 'destek', 'ol', 'paylaş', 'adres', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>2024-11-17 16:28:31</td>\n",
       "      <td>aMMo</td>\n",
       "      <td>tatavaypmadvmet</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>teyi̇tli̇ lütfen yardim\\nemlak bank evleri 1. ...</td>\n",
       "      <td>teyi̇tli̇ lütfen yardim\\nemlak bank evleri 1. ...</td>\n",
       "      <td>teyi̇tli̇ lütfen yardim emlak bank evleri  kıs...</td>\n",
       "      <td>teyi̇tli̇ lütfen yardim emlak evleri kısım blo...</td>\n",
       "      <td>teyi̇tli̇ lütfen yardım emlak evleri kısım blo...</td>\n",
       "      <td>['teyi̇tli̇', 'lütfen', 'yardım', 'emlak', 'ev...</td>\n",
       "      <td>['teyi̇tli̇', 'yardım', 'emlak', 'evleri', 'kı...</td>\n",
       "      <td>['teyi̇tli̇', 'yar', 'emlak', 'ev', 'kısım', '...</td>\n",
       "      <td>['teyi̇tli̇', 'yardım', 'ev', 'blok', 'hatay',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>2024-11-17 16:28:31</td>\n",
       "      <td>N’Y</td>\n",
       "      <td>nyonyedi</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>05324707903 babalatv\\n\\nwhastapp hatti tüm böl...</td>\n",
       "      <td>05324707903 babalatv\\n\\nwhastapp hatti tüm böl...</td>\n",
       "      <td>babalatv  whastapp hatti tüm bölgelerdeki yar...</td>\n",
       "      <td>babalatv whastapp hatti tüm bölgelerdeki yardi...</td>\n",
       "      <td>babalatv Whatsapp hattı tüm bölgelerdeki yardı...</td>\n",
       "      <td>['babalatv', 'Whatsapp', 'hattı', 'tüm', 'bölg...</td>\n",
       "      <td>['babalatv', 'Whatsapp', 'hattı', 'bölgelerdek...</td>\n",
       "      <td>['babalatv', 'whatsapp', 'hatt', 'bölge', 'yar...</td>\n",
       "      <td>['babalatv', 'bölge', 'yardım', 'babalatv', 'o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>2024-11-17 16:28:42</td>\n",
       "      <td>Casuel</td>\n",
       "      <td>allin39949323</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>işık olmadığı için çalışmalar durmak zorunda k...</td>\n",
       "      <td>işık olmadığı için çalışmalar durmak zorunda k...</td>\n",
       "      <td>işık olmadığı için çalışmalar durmak zorunda k...</td>\n",
       "      <td>işık olmadığı için çalışmalar durmak zorunda k...</td>\n",
       "      <td>ışık olmadığı için çalışmalar durmak zorunda k...</td>\n",
       "      <td>['ışık', 'olmadığı', 'için', 'çalışmalar', 'du...</td>\n",
       "      <td>['ışık', 'çalışmalar', 'durmak', 'zorunda', 'k...</td>\n",
       "      <td>['ışık', 'çalış', 'dur', 'zor', 'kal', 'acilen...</td>\n",
       "      <td>['ışık', 'çalış', 'dur', 'zor', 'kal', 'acilen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>2024-11-17 16:28:42</td>\n",
       "      <td>Mekatronik1907</td>\n",
       "      <td>mekatronik1907</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>tuğba-uğur-nil kılınç\\n\\ngeneral şükrü kanatlı...</td>\n",
       "      <td>tuğba-uğur-nil kılınç\\n\\ngeneral şükrü kanatlı...</td>\n",
       "      <td>tuğbauğurnil kılınç  general şükrü kanatlı yav...</td>\n",
       "      <td>tuğbauğurnil kılınç şükrü kanatlı yavuz selim ...</td>\n",
       "      <td>tuğbaugürnil kılınç şükrü kanatlı yavuz selim ...</td>\n",
       "      <td>['tuğbaugürnil', 'kılınç', 'şükrü', 'kanatlı',...</td>\n",
       "      <td>['tuğbaugürnil', 'kılınç', 'şükrü', 'kanatlı',...</td>\n",
       "      <td>['tuğbaugürnil', 'kılınç', 'şükr', 'kanat', 'y...</td>\n",
       "      <td>['şükr', 'kanat', 'antakyahatay', 'acil', 'yar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2102 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp        username           handle   tweet_date  \\\n",
       "0     2024-11-03 19:38:25     Orhan Şevik      orhan_sevik  Feb 8, 2023   \n",
       "1     2024-11-03 19:38:25         nursena         nurssxx_  Feb 8, 2023   \n",
       "2     2024-11-03 19:38:25      Elif ŞEKER      55ellllllif  Feb 8, 2023   \n",
       "3     2024-11-03 19:38:25           Yusuf      yusufaltuns  Feb 8, 2023   \n",
       "4     2024-11-03 19:38:25     𝐎̈𝐳𝐠𝐮̈𝐫 𝐑𝐚𝐧         dryghtn2  Feb 8, 2023   \n",
       "...                   ...             ...              ...          ...   \n",
       "2097  2024-11-17 16:28:31       KEREMCGRC       keremcgrci  Feb 8, 2023   \n",
       "2098  2024-11-17 16:28:31            aMMo  tatavaypmadvmet  Feb 8, 2023   \n",
       "2099  2024-11-17 16:28:31             N’Y         nyonyedi  Feb 8, 2023   \n",
       "2100  2024-11-17 16:28:42          Casuel    allin39949323  Feb 8, 2023   \n",
       "2101  2024-11-17 16:28:42  Mekatronik1907   mekatronik1907  Feb 8, 2023   \n",
       "\n",
       "                                                content  \\\n",
       "0     @haluklevent\\n @oguzhanugur\\n  hepinizden alla...   \n",
       "1     allahım nolur sen onlara dayanma gücü ver #deprem   \n",
       "2     replying to \\n@haluklevent\\n @danlabilic\\n and...   \n",
       "3          tuğba södekoğlu kovulsun \\n@showtv\\n #deprem   \n",
       "4     arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...   \n",
       "...                                                 ...   \n",
       "2097  yardım ağı´na destek ol! \\n\\ntwitter'da paylaş...   \n",
       "2098  teyi̇tli̇ lütfen yardim\\nemlak bank evleri 1. ...   \n",
       "2099  05324707903 babalatv\\n\\nwhastapp hatti tüm böl...   \n",
       "2100  işık olmadığı için çalışmalar durmak zorunda k...   \n",
       "2101  tuğba-uğur-nil kılınç\\n\\ngeneral şükrü kanatlı...   \n",
       "\n",
       "                                      content_ment_link  \\\n",
       "0     \\n \\n  hepinizden allah razı olsun iyiki varsı...   \n",
       "1     allahım nolur sen onlara dayanma gücü ver #deprem   \n",
       "2     replying to \\n\\n \\n and \\n_harun\\nadıyamanda a...   \n",
       "3                 tuğba södekoğlu kovulsun \\n\\n #deprem   \n",
       "4     arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...   \n",
       "...                                                 ...   \n",
       "2097  yardım ağı´na destek ol! \\n\\ntwitter'da paylaş...   \n",
       "2098  teyi̇tli̇ lütfen yardim\\nemlak bank evleri 1. ...   \n",
       "2099  05324707903 babalatv\\n\\nwhastapp hatti tüm böl...   \n",
       "2100  işık olmadığı için çalışmalar durmak zorunda k...   \n",
       "2101  tuğba-uğur-nil kılınç\\n\\ngeneral şükrü kanatlı...   \n",
       "\n",
       "                                       content_wo_punct  \\\n",
       "0          hepinizden allah razı olsun iyiki varsını...   \n",
       "1      allahım nolur sen onlara dayanma gücü ver deprem   \n",
       "2     replying to      and  harun adıyamanda ali taş...   \n",
       "3                    tuğba södekoğlu kovulsun    deprem   \n",
       "4     arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...   \n",
       "...                                                 ...   \n",
       "2097  yardım ağı´na destek ol   twitterda paylaşılan...   \n",
       "2098  teyi̇tli̇ lütfen yardim emlak bank evleri  kıs...   \n",
       "2099   babalatv  whastapp hatti tüm bölgelerdeki yar...   \n",
       "2100  işık olmadığı için çalışmalar durmak zorunda k...   \n",
       "2101  tuğbauğurnil kılınç  general şükrü kanatlı yav...   \n",
       "\n",
       "                             content_wo_removed_english  \\\n",
       "0     hepinizden allah razı olsun iyiki varsınız deprem   \n",
       "1          allahım nolur onlara dayanma gücü ver deprem   \n",
       "2      harun adıyamanda ali taşı mahallesi sokak hic...   \n",
       "3                       tuğba södekoğlu kovulsun deprem   \n",
       "4     arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...   \n",
       "...                                                 ...   \n",
       "2097  yardım ağı´na destek ol twitterda paylaşılan a...   \n",
       "2098  teyi̇tli̇ lütfen yardim emlak evleri kısım blo...   \n",
       "2099  babalatv whastapp hatti tüm bölgelerdeki yardi...   \n",
       "2100  işık olmadığı için çalışmalar durmak zorunda k...   \n",
       "2101  tuğbauğurnil kılınç şükrü kanatlı yavuz selim ...   \n",
       "\n",
       "                                   content_wo_normalize  \\\n",
       "0     hepinizden allah razı olsun iyi ki varsınız de...   \n",
       "1        allahım ne olur onlara dayanma gücü ver deprem   \n",
       "2     harun adıyamanda ali taşı mahallesi sokak hicr...   \n",
       "3                       tuğba sodekoğlu kovulsun deprem   \n",
       "4     arkadaşımıza ulaşamıyoruz kahramanmaraş elbist...   \n",
       "...                                                 ...   \n",
       "2097  yardım ağına destek ol twitterda paylaşılan ad...   \n",
       "2098  teyi̇tli̇ lütfen yardım emlak evleri kısım blo...   \n",
       "2099  babalatv Whatsapp hattı tüm bölgelerdeki yardı...   \n",
       "2100  ışık olmadığı için çalışmalar durmak zorunda k...   \n",
       "2101  tuğbaugürnil kılınç şükrü kanatlı yavuz selim ...   \n",
       "\n",
       "                                    content_wo_tokenize  \\\n",
       "0     ['hepinizden', 'allah', 'razı', 'olsun', 'iyi'...   \n",
       "1     ['allahım', 'ne', 'olur', 'onlara', 'dayanma',...   \n",
       "2     ['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...   \n",
       "3          ['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']   \n",
       "4     ['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...   \n",
       "...                                                 ...   \n",
       "2097  ['yardım', 'ağına', 'destek', 'ol', 'twitterda...   \n",
       "2098  ['teyi̇tli̇', 'lütfen', 'yardım', 'emlak', 'ev...   \n",
       "2099  ['babalatv', 'Whatsapp', 'hattı', 'tüm', 'bölg...   \n",
       "2100  ['ışık', 'olmadığı', 'için', 'çalışmalar', 'du...   \n",
       "2101  ['tuğbaugürnil', 'kılınç', 'şükrü', 'kanatlı',...   \n",
       "\n",
       "                                        content_wo_stop  \\\n",
       "0          ['hepinizden', 'razı', 'varsınız', 'deprem']   \n",
       "1        ['onlara', 'dayanma', 'gücü', 'ver', 'deprem']   \n",
       "2     ['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...   \n",
       "3          ['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']   \n",
       "4     ['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...   \n",
       "...                                                 ...   \n",
       "2097  ['yardım', 'ağına', 'destek', 'ol', 'twitterda...   \n",
       "2098  ['teyi̇tli̇', 'yardım', 'emlak', 'evleri', 'kı...   \n",
       "2099  ['babalatv', 'Whatsapp', 'hattı', 'bölgelerdek...   \n",
       "2100  ['ışık', 'çalışmalar', 'durmak', 'zorunda', 'k...   \n",
       "2101  ['tuğbaugürnil', 'kılınç', 'şükrü', 'kanatlı',...   \n",
       "\n",
       "                                  content_wo_lemmatized  \\\n",
       "0                      ['hep', 'razı', 'var', 'deprem']   \n",
       "1                ['o', 'dayan', 'güc', 'ver', 'deprem']   \n",
       "2     ['haru', 'adıyaman', 'ali', 'taşı', 'mahalle',...   \n",
       "3               ['tuğba', 'sodekoğlu', 'kov', 'deprem']   \n",
       "4     ['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...   \n",
       "...                                                 ...   \n",
       "2097  ['yar', 'ağın', 'destek', 'ol', 'twitter', 'pa...   \n",
       "2098  ['teyi̇tli̇', 'yar', 'emlak', 'ev', 'kısım', '...   \n",
       "2099  ['babalatv', 'whatsapp', 'hatt', 'bölge', 'yar...   \n",
       "2100  ['ışık', 'çalış', 'dur', 'zor', 'kal', 'acilen...   \n",
       "2101  ['tuğbaugürnil', 'kılınç', 'şükr', 'kanat', 'y...   \n",
       "\n",
       "                                  content_no_rare_words  \n",
       "0                      ['hep', 'razı', 'var', 'deprem']  \n",
       "1                ['o', 'dayan', 'güc', 'ver', 'deprem']  \n",
       "2     ['adıyaman', 'ali', 'taşı', 'mahalle', 'sokak'...  \n",
       "3                                            ['deprem']  \n",
       "4     ['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...  \n",
       "...                                                 ...  \n",
       "2097  ['yardım', 'destek', 'ol', 'paylaş', 'adres', ...  \n",
       "2098  ['teyi̇tli̇', 'yardım', 'ev', 'blok', 'hatay',...  \n",
       "2099  ['babalatv', 'bölge', 'yardım', 'babalatv', 'o...  \n",
       "2100  ['ışık', 'çalış', 'dur', 'zor', 'kal', 'acilen...  \n",
       "2101  ['şükr', 'kanat', 'antakyahatay', 'acil', 'yar...  \n",
       "\n",
       "[2102 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_keywords = [\"mahallesi\", \"caddesi\", \"sokak\", \"bulvarı\", \"köyü\", \"yolu\", \"mevkii\", \"mah.\", \"cad.\", \"sok.\", \"bul.\", \"köy.\", \"yol.\", \"mek.\",\"apartman\",\"apt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_from_text(text):\n",
    "    if not text.strip():\n",
    "        return None \n",
    "    result = pipe(text) \n",
    "    location = \"\" \n",
    "    \n",
    "    for entity in result:\n",
    "        token = entity['word']\n",
    "        label = entity['entity']\n",
    "\n",
    "        if 'LOC' in label:  \n",
    "            if not token.startswith('##'):  \n",
    "                if location:  \n",
    "                    location += \" \" \n",
    "                location += token\n",
    "            else:\n",
    "                location += token.lstrip('##')  \n",
    "    words = text.split()  \n",
    "    for i, word in enumerate(words):\n",
    "        if any(loc in word for loc in loc_tokens):\n",
    "            location += word + \" \" \n",
    "        if any(keyword in word.lower() for keyword in address_keywords):\n",
    "            location += word + \" \" \n",
    "\n",
    "    return location.strip() if location else None\n",
    "df['location'] = df['content_wo_normalize'].apply(get_location_from_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latitude & longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adres: hatay antakya cebrail mahallesi\n",
      "Enlem: 36.2063404, Boylam: 36.1565635\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "# Adres olarak tam metni belirtiyoruz\n",
    "address = \"hatay antakya cebrail mahallesi\"\n",
    "\n",
    "# Adresi enlem ve boylama çevir\n",
    "location = geolocator.geocode(address)\n",
    "\n",
    "if location:\n",
    "    print(f\"Adres: {address}\")\n",
    "    print(f\"Enlem: {location.latitude}, Boylam: {location.longitude}\")\n",
    "else:\n",
    "    print(f\"Adres bulunamadı: {address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "def get_coordinates(address):\n",
    "    if not address or pd.isna(address):\n",
    "        return None, None\n",
    "    try:\n",
    "        location = geolocator.geocode(address, timeout=10)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['latitude', 'longitude']] = df['location'].apply(\n",
    "    lambda x: pd.Series(get_coordinates(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_with_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "m = folium.Map(location=[41.0369, 28.9850], zoom_start=6)\n",
    "\n",
    "for _, row in df_valid.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['location'] \n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"harita.html\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
