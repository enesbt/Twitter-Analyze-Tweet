{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from itertools import permutations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enes\\anaconda3\\envs\\gputorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", model=\"akdeniz27/bert-base-turkish-cased-ner\",device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraş, Etiket: B-LOC\n",
      "Token: elbis, Etiket: B-LOC\n",
      "Token: ##tan, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##ınar, Etiket: B-LOC\n",
      "Token: ##başı, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##ınar, Etiket: B-LOC\n",
      "Token: ##başı, Etiket: B-LOC\n",
      "Token: cem, Etiket: B-PER\n",
      "Token: ##re, Etiket: B-PER\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraş, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "kahramanmaraş elbistan pınarbaşı mahallesi pınarbaşı caddesi kahramanmaraş\n"
     ]
    }
   ],
   "source": [
    "text = \"arkadaşımıza ulaşamıyoruz kahramanmaraş elbistan pınarbaşı mahallesi pınarbaşı caddesi cemre yapıcı kahramanmaraş deprem\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "\n",
    "location = \"\"\n",
    "address_keywords = [\"mahallesi\", \"caddesi\", \"sokak\", \"bulvarı\", \"köyü\", \"yolu\", \"mevkii\", \"mah.\", \"cad.\", \"sok.\", \"bul.\", \"köy.\", \"yol.\", \"mek.\",\"apartman\",\"apt\"]\n",
    "\n",
    "loc_tokens = []\n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "\n",
    "    if \"LOC\" in label:  \n",
    "        if token.startswith(\"##\"):\n",
    "            loc_tokens[-1] += token.lstrip(\"##\")  \n",
    "        else:\n",
    "            loc_tokens.append(token)\n",
    "\n",
    "words = text.split()  \n",
    "for i, word in enumerate(words):\n",
    "    if any(loc in word for loc in loc_tokens):\n",
    "        location += word + \" \" \n",
    "    if any(keyword in word.lower() for keyword in address_keywords):\n",
    "            location += word + \" \" \n",
    "loc_tokens\n",
    "print(\"\\nLocation:\")\n",
    "print(location.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: ak, Etiket: B-LOC\n",
      "Token: ##asy, Etiket: B-LOC\n",
      "Token: ##a, Etiket: B-LOC\n",
      "Token: bal, Etiket: I-PER\n",
      "Token: ##cı, Etiket: I-PER\n",
      "Token: sar, Etiket: B-LOC\n",
      "Token: ##a, Etiket: B-LOC\n",
      "Token: hata, Etiket: B-LOC\n",
      "Token: ##y, Etiket: B-LOC\n",
      "Token: ant, Etiket: B-LOC\n",
      "Token: ##ak, Etiket: B-LOC\n",
      "Token: ##ya, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "akasya sara hatay antakya\n"
     ]
    }
   ],
   "source": [
    "text = \"çocuk enkazın altında ai̇leşi̇ i̇ki̇ gündür acil müdahale edi̇lmesi̇ni̇ bekliyor adres akasya mahallesi şükrü balcı caddesi sara apartmanı hatay antakya iletişim deprem sondakikadeprem acil acildeprem\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "\n",
    "location = \"\"\n",
    "\n",
    "loc_tokens = []\n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "\n",
    "    # LOC etiketli tokenları topla\n",
    "    if 'LOC' in label:  \n",
    "            if not token.startswith('##'):  \n",
    "                if location:  \n",
    "                    location += \" \" \n",
    "                location += token\n",
    "            else:\n",
    "                location += token.lstrip('##')  \n",
    "\n",
    "\n",
    "print(\"\\nLocation:\")\n",
    "print(location.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adres: hatay esenlik mahallesi\n",
      "Enlem: 36.2059571, Boylam: 36.1478466\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "# Adres olarak tam metni belirtiyoruz\n",
    "address = \"hatay esenlik mahallesi\"\n",
    "\n",
    "# Adresi enlem ve boylama çevir\n",
    "location = geolocator.geocode(address)\n",
    "\n",
    "if location:\n",
    "    print(f\"Adres: {address}\")\n",
    "    print(f\"Enlem: {location.latitude}, Boylam: {location.longitude}\")\n",
    "else:\n",
    "    print(f\"Adres bulunamadı: {address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_ment_link</th>\n",
       "      <th>content_wo_punct</th>\n",
       "      <th>content_wo_removed_english</th>\n",
       "      <th>content_wo_normalize</th>\n",
       "      <th>content_wo_tokenize</th>\n",
       "      <th>content_wo_stop</th>\n",
       "      <th>content_wo_lemmatized</th>\n",
       "      <th>content_no_rare_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Orhan Şevik</td>\n",
       "      <td>orhan_sevik</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>@haluklevent\\n @oguzhanugur\\n  hepinizden alla...</td>\n",
       "      <td>\\n \\n  hepinizden allah razı olsun iyiki varsı...</td>\n",
       "      <td>hepinizden allah razı olsun iyiki varsını...</td>\n",
       "      <td>hepinizden allah razı olsun iyiki varsınız deprem</td>\n",
       "      <td>hepinizden allah razı olsun iyi ki varsınız de...</td>\n",
       "      <td>['hepinizden', 'allah', 'razı', 'olsun', 'iyi'...</td>\n",
       "      <td>['hepinizden', 'razı', 'varsınız', 'deprem']</td>\n",
       "      <td>['hep', 'razı', 'var', 'deprem']</td>\n",
       "      <td>['hep', 'razı', 'var', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>nursena</td>\n",
       "      <td>nurssxx_</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>allahım nolur sen onlara dayanma gücü ver #deprem</td>\n",
       "      <td>allahım nolur sen onlara dayanma gücü ver #deprem</td>\n",
       "      <td>allahım nolur sen onlara dayanma gücü ver deprem</td>\n",
       "      <td>allahım nolur onlara dayanma gücü ver deprem</td>\n",
       "      <td>allahım ne olur onlara dayanma gücü ver deprem</td>\n",
       "      <td>['allahım', 'ne', 'olur', 'onlara', 'dayanma',...</td>\n",
       "      <td>['onlara', 'dayanma', 'gücü', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'güc', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'güc', 'ver', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Elif ŞEKER</td>\n",
       "      <td>55ellllllif</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>replying to \\n@haluklevent\\n @danlabilic\\n and...</td>\n",
       "      <td>replying to \\n\\n \\n and \\n_harun\\nadıyamanda a...</td>\n",
       "      <td>replying to      and  harun adıyamanda ali taş...</td>\n",
       "      <td>harun adıyamanda ali taşı mahallesi sokak hic...</td>\n",
       "      <td>harun adıyamanda ali taşı mahallesi sokak hicr...</td>\n",
       "      <td>['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...</td>\n",
       "      <td>['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...</td>\n",
       "      <td>['haru', 'adıyaman', 'ali', 'taşı', 'mahalle',...</td>\n",
       "      <td>['adıyaman', 'ali', 'taşı', 'mahalle', 'sokak'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Yusuf</td>\n",
       "      <td>yusufaltuns</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>tuğba södekoğlu kovulsun \\n@showtv\\n #deprem</td>\n",
       "      <td>tuğba södekoğlu kovulsun \\n\\n #deprem</td>\n",
       "      <td>tuğba södekoğlu kovulsun    deprem</td>\n",
       "      <td>tuğba södekoğlu kovulsun deprem</td>\n",
       "      <td>tuğba sodekoğlu kovulsun deprem</td>\n",
       "      <td>['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuğba', 'sodekoğlu', 'kov', 'deprem']</td>\n",
       "      <td>['deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>𝐎̈𝐳𝐠𝐮̈𝐫 𝐑𝐚𝐧</td>\n",
       "      <td>dryghtn2</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...</td>\n",
       "      <td>arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...</td>\n",
       "      <td>arkadaşımıza ulaşamıyoruz kahramanmaraş elbist...</td>\n",
       "      <td>['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...</td>\n",
       "      <td>['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     username       handle   tweet_date  \\\n",
       "0  2024-11-03 19:38:25  Orhan Şevik  orhan_sevik  Feb 8, 2023   \n",
       "1  2024-11-03 19:38:25      nursena     nurssxx_  Feb 8, 2023   \n",
       "2  2024-11-03 19:38:25   Elif ŞEKER  55ellllllif  Feb 8, 2023   \n",
       "3  2024-11-03 19:38:25        Yusuf  yusufaltuns  Feb 8, 2023   \n",
       "4  2024-11-03 19:38:25  𝐎̈𝐳𝐠𝐮̈𝐫 𝐑𝐚𝐧     dryghtn2  Feb 8, 2023   \n",
       "\n",
       "                                             content  \\\n",
       "0  @haluklevent\\n @oguzhanugur\\n  hepinizden alla...   \n",
       "1  allahım nolur sen onlara dayanma gücü ver #deprem   \n",
       "2  replying to \\n@haluklevent\\n @danlabilic\\n and...   \n",
       "3       tuğba södekoğlu kovulsun \\n@showtv\\n #deprem   \n",
       "4  arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...   \n",
       "\n",
       "                                   content_ment_link  \\\n",
       "0  \\n \\n  hepinizden allah razı olsun iyiki varsı...   \n",
       "1  allahım nolur sen onlara dayanma gücü ver #deprem   \n",
       "2  replying to \\n\\n \\n and \\n_harun\\nadıyamanda a...   \n",
       "3              tuğba södekoğlu kovulsun \\n\\n #deprem   \n",
       "4  arkadaşimiza ulaşamiyoruz\\nkahramanmaraş elbis...   \n",
       "\n",
       "                                    content_wo_punct  \\\n",
       "0       hepinizden allah razı olsun iyiki varsını...   \n",
       "1   allahım nolur sen onlara dayanma gücü ver deprem   \n",
       "2  replying to      and  harun adıyamanda ali taş...   \n",
       "3                 tuğba södekoğlu kovulsun    deprem   \n",
       "4  arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...   \n",
       "\n",
       "                          content_wo_removed_english  \\\n",
       "0  hepinizden allah razı olsun iyiki varsınız deprem   \n",
       "1       allahım nolur onlara dayanma gücü ver deprem   \n",
       "2   harun adıyamanda ali taşı mahallesi sokak hic...   \n",
       "3                    tuğba södekoğlu kovulsun deprem   \n",
       "4  arkadaşimiza ulaşamiyoruz kahramanmaraş elbist...   \n",
       "\n",
       "                                content_wo_normalize  \\\n",
       "0  hepinizden allah razı olsun iyi ki varsınız de...   \n",
       "1     allahım ne olur onlara dayanma gücü ver deprem   \n",
       "2  harun adıyamanda ali taşı mahallesi sokak hicr...   \n",
       "3                    tuğba sodekoğlu kovulsun deprem   \n",
       "4  arkadaşımıza ulaşamıyoruz kahramanmaraş elbist...   \n",
       "\n",
       "                                 content_wo_tokenize  \\\n",
       "0  ['hepinizden', 'allah', 'razı', 'olsun', 'iyi'...   \n",
       "1  ['allahım', 'ne', 'olur', 'onlara', 'dayanma',...   \n",
       "2  ['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...   \n",
       "3       ['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']   \n",
       "4  ['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...   \n",
       "\n",
       "                                     content_wo_stop  \\\n",
       "0       ['hepinizden', 'razı', 'varsınız', 'deprem']   \n",
       "1     ['onlara', 'dayanma', 'gücü', 'ver', 'deprem']   \n",
       "2  ['harun', 'adıyamanda', 'ali', 'taşı', 'mahall...   \n",
       "3       ['tuğba', 'sodekoğlu', 'kovulsun', 'deprem']   \n",
       "4  ['arkadaşımıza', 'ulaşamıyoruz', 'kahramanmara...   \n",
       "\n",
       "                               content_wo_lemmatized  \\\n",
       "0                   ['hep', 'razı', 'var', 'deprem']   \n",
       "1             ['o', 'dayan', 'güc', 'ver', 'deprem']   \n",
       "2  ['haru', 'adıyaman', 'ali', 'taşı', 'mahalle',...   \n",
       "3            ['tuğba', 'sodekoğlu', 'kov', 'deprem']   \n",
       "4  ['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...   \n",
       "\n",
       "                               content_no_rare_words  \n",
       "0                   ['hep', 'razı', 'var', 'deprem']  \n",
       "1             ['o', 'dayan', 'güc', 'ver', 'deprem']  \n",
       "2  ['adıyaman', 'ali', 'taşı', 'mahalle', 'sokak'...  \n",
       "3                                         ['deprem']  \n",
       "4  ['arkadaş', 'ulaş', 'kahramanmaraş', 'elbistan...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/TweetAnalyzeResult.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize(text):\n",
    "    words = text.split()\n",
    "    capitalized_words = [word.capitalize() for word in words]\n",
    "    capitalized_text = ' '.join(capitalized_words)\n",
    "    return capitalized_text\n",
    "\n",
    "df['content_wo_tokenize_capitalize'] = df['content_wo_normalize'].apply(capitalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_from_text(text):\n",
    "    if not text.strip():\n",
    "        return None \n",
    "    result = pipe(text) \n",
    "    location = \"\" \n",
    "    \n",
    "    for entity in result:\n",
    "        token = entity['word']\n",
    "        label = entity['entity']\n",
    "\n",
    "        if 'LOC' in label:  \n",
    "            if not token.startswith('##'):  \n",
    "                if location:  \n",
    "                    location += \" \" \n",
    "                location += token\n",
    "            else:\n",
    "                location += token.lstrip('##')  \n",
    "   \n",
    "\n",
    "    return location.strip() if location else None\n",
    "\n",
    "df['location'] = df['content_wo_tokenize_capitalize'].apply(get_location_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    unique_words = list(set(words))\n",
    "    unique_words.sort(key=lambda x: words.index(x))  \n",
    "    return ' '.join(unique_words)\n",
    "\n",
    "df['location_unique'] = df['location'].apply(remove_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_location'] = df['location_unique'].apply(lambda x: x.split() if pd.notnull(x) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "def get_coordinates(address):\n",
    "    if not address or pd.isna(address):\n",
    "        return None, None\n",
    "    try:\n",
    "        location = geolocator.geocode(address, timeout=10)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.388756, 35.916489)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coordinates(\"Hatay Arsuz gözcüler \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkiye_adres = pd.read_excel(\"../data/ililcemahalle.xlsx\")\n",
    "\n",
    "turkiye_adres[\"il\"] = turkiye_adres[\"il\"].str.strip()\n",
    "turkiye_adres[\"ilçe\"] = turkiye_adres[\"ilçe\"].str.strip()\n",
    "turkiye_adres[\"semt_bucak_belde\"] = turkiye_adres[\"semt_bucak_belde\"].str.strip()\n",
    "turkiye_adres[\"Mahalle\"] = turkiye_adres[\"Mahalle\"].str.strip()\n",
    "\n",
    "iller = set(turkiye_adres[\"il\"].str.lower().unique())\n",
    "ilceler = set(turkiye_adres[\"ilçe\"].str.lower().unique())\n",
    "semtler = set(turkiye_adres[\"semt_bucak_belde\"].str.lower().unique())\n",
    "mahalleler = set(turkiye_adres[\"Mahalle\"].str.lower().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_in_set(token, keyword_set):\n",
    "    return [keyword for keyword in keyword_set if keyword in token]\n",
    "\n",
    "def clean_address(address):\n",
    "    return [word for word in address if 'hal' not in word.lower()]\n",
    "\n",
    "def build_address_from_tokens(tokens):\n",
    "    if not isinstance(tokens, list) or not tokens or len(tokens) == 1:\n",
    "        return None  \n",
    "    \n",
    "    found_parts = []  \n",
    "    tokens = [token.lower() for token in tokens]  \n",
    "\n",
    "    for token in tokens:\n",
    "        if (matched_parts := contains_in_set(token, iller)):\n",
    "            if matched_parts[0] not in found_parts:\n",
    "                found_parts.append(matched_parts[0])  \n",
    "        elif (matched_parts := contains_in_set(token, ilceler)):\n",
    "            if matched_parts[0] not in found_parts:\n",
    "                found_parts.append(matched_parts[0])\n",
    "\n",
    "    found_parts = clean_address(found_parts)\n",
    "    \n",
    "    return \" \".join(found_parts) if found_parts else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"address\"] = df[\"tokenized_location\"].apply(build_address_from_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coordinates(row):\n",
    "    if pd.notna(row['address']) and len(row['address'].split()) > 1: \n",
    "        return get_coordinates(row['address'])\n",
    "    return None, None  \n",
    "\n",
    "\n",
    "df[\"latitude\"], df[\"longitude\"] = zip(*df.apply(update_coordinates, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sehirler = [\n",
    "    \"Adana\", \"Adıyaman\", \"Afyon\", \"Ağrı\", \"Amasya\", \"Ankara\", \"Antalya\", \"Artvin\",\n",
    "    \"Aydın\", \"Balıkesir\", \"Bilecik\", \"Bingöl\", \"Bitlis\", \"Bolu\", \"Burdur\", \"Bursa\", \"Çanakkale\",\n",
    "    \"Çankırı\", \"Çorum\", \"Denizli\", \"Diyarbakır\", \"Edirne\", \"Elazığ\", \"Erzincan\", \"Erzurum\", \n",
    "    \"Eskişehir\", \"Gaziantep\", \"Giresun\", \"Gümüşhane\", \"Hakkari\", \"Hatay\", \"Isparta\", \"Mersin\",\n",
    "    \"İstanbul\", \"İzmir\", \"Kars\", \"Kastamonu\", \"Kayseri\", \"Kırklareli\", \"Kırşehir\", \"Kocaeli\",\n",
    "    \"Konya\", \"Kütahya\", \"Malatya\", \"Manisa\", \"Kahramanmaraş\", \"Mardin\", \"Muğla\", \"Muş\", \n",
    "    \"Nevşehir\", \"Niğde\", \"Ordu\", \"Rize\", \"Sakarya\", \"Samsun\", \"Siirt\", \"Sinop\", \"Sivas\", \n",
    "    \"Tekirdağ\", \"Tokat\", \"Trabzon\", \"Tunceli\", \"Şanlıurfa\", \"Uşak\", \"Van\", \"Yozgat\", \n",
    "    \"Zonguldak\", \"Aksaray\", \"Bayburt\", \"Karaman\", \"Kırıkkale\", \"Batman\", \"Şırnak\", \n",
    "    \"Bartın\", \"Ardahan\", \"Iğdır\", \"Yalova\", \"Karabük\", \"Kilis\", \"Osmaniye\", \"Düzce\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cities(tokens, city_list):\n",
    "    if not tokens:  \n",
    "        return []\n",
    "    matches = []\n",
    "    for city in city_list:\n",
    "        for token in tokens:\n",
    "            if re.search(r'\\b' + re.escape(city) + r'\\b', token, re.IGNORECASE): \n",
    "                matches.append(city)\n",
    "    return list(set(matches))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'] = df['tokenized_location'].apply(lambda x: extract_cities(x, sehirler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get latitude & longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_city_coordinates(row):\n",
    "    if isinstance(row['city'], list) and len(row['city']) > 0 and pd.notna(row['city'][0]):\n",
    "        return get_coordinates(row['city'][0])\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"latitude_city\"], df[\"longitude_city\"] = zip(*df.apply(update_city_coordinates, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../results/LocationResult.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
