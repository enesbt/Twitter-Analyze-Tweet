{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from itertools import permutations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Enes\\anaconda3\\envs\\gputorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", model=\"akdeniz27/bert-base-turkish-cased-ner\",device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraÅŸ, Etiket: B-LOC\n",
      "Token: elbis, Etiket: B-LOC\n",
      "Token: ##tan, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##Ä±nar, Etiket: B-LOC\n",
      "Token: ##baÅŸÄ±, Etiket: B-LOC\n",
      "Token: p, Etiket: B-LOC\n",
      "Token: ##Ä±nar, Etiket: B-LOC\n",
      "Token: ##baÅŸÄ±, Etiket: B-LOC\n",
      "Token: cem, Etiket: B-PER\n",
      "Token: ##re, Etiket: B-PER\n",
      "Token: kahraman, Etiket: B-LOC\n",
      "Token: ##maraÅŸ, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "kahramanmaraÅŸ elbistan pÄ±narbaÅŸÄ± mahallesi pÄ±narbaÅŸÄ± caddesi kahramanmaraÅŸ\n"
     ]
    }
   ],
   "source": [
    "text = \"arkadaÅŸÄ±mÄ±za ulaÅŸamÄ±yoruz kahramanmaraÅŸ elbistan pÄ±narbaÅŸÄ± mahallesi pÄ±narbaÅŸÄ± caddesi cemre yapÄ±cÄ± kahramanmaraÅŸ deprem\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "\n",
    "location = \"\"\n",
    "address_keywords = [\"mahallesi\", \"caddesi\", \"sokak\", \"bulvarÄ±\", \"kÃ¶yÃ¼\", \"yolu\", \"mevkii\", \"mah.\", \"cad.\", \"sok.\", \"bul.\", \"kÃ¶y.\", \"yol.\", \"mek.\",\"apartman\",\"apt\"]\n",
    "\n",
    "loc_tokens = []\n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "\n",
    "    if \"LOC\" in label:  \n",
    "        if token.startswith(\"##\"):\n",
    "            loc_tokens[-1] += token.lstrip(\"##\")  \n",
    "        else:\n",
    "            loc_tokens.append(token)\n",
    "\n",
    "words = text.split()  \n",
    "for i, word in enumerate(words):\n",
    "    if any(loc in word for loc in loc_tokens):\n",
    "        location += word + \" \" \n",
    "    if any(keyword in word.lower() for keyword in address_keywords):\n",
    "            location += word + \" \" \n",
    "loc_tokens\n",
    "print(\"\\nLocation:\")\n",
    "print(location.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ve Etiketler:\n",
      "Token: ak, Etiket: B-LOC\n",
      "Token: ##asy, Etiket: B-LOC\n",
      "Token: ##a, Etiket: B-LOC\n",
      "Token: bal, Etiket: I-PER\n",
      "Token: ##cÄ±, Etiket: I-PER\n",
      "Token: sar, Etiket: B-LOC\n",
      "Token: ##a, Etiket: B-LOC\n",
      "Token: hata, Etiket: B-LOC\n",
      "Token: ##y, Etiket: B-LOC\n",
      "Token: ant, Etiket: B-LOC\n",
      "Token: ##ak, Etiket: B-LOC\n",
      "Token: ##ya, Etiket: B-LOC\n",
      "\n",
      "Location:\n",
      "akasya sara hatay antakya\n"
     ]
    }
   ],
   "source": [
    "text = \"Ã§ocuk enkazÄ±n altÄ±nda aiÌ‡leÅŸiÌ‡ iÌ‡kiÌ‡ gÃ¼ndÃ¼r acil mÃ¼dahale ediÌ‡lmesiÌ‡niÌ‡ bekliyor adres akasya mahallesi ÅŸÃ¼krÃ¼ balcÄ± caddesi sara apartmanÄ± hatay antakya iletiÅŸim deprem sondakikadeprem acil acildeprem\"\n",
    "\n",
    "result = pipe(text)\n",
    "\n",
    "print(\"Token ve Etiketler:\")\n",
    "\n",
    "location = \"\"\n",
    "\n",
    "loc_tokens = []\n",
    "for entity in result:\n",
    "    token = entity['word']\n",
    "    label = entity['entity']\n",
    "    print(f\"Token: {token}, Etiket: {label}\")\n",
    "\n",
    "    # LOC etiketli tokenlarÄ± topla\n",
    "    if 'LOC' in label:  \n",
    "            if not token.startswith('##'):  \n",
    "                if location:  \n",
    "                    location += \" \" \n",
    "                location += token\n",
    "            else:\n",
    "                location += token.lstrip('##')  \n",
    "\n",
    "\n",
    "print(\"\\nLocation:\")\n",
    "print(location.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adres: hatay esenlik mahallesi\n",
      "Enlem: 36.2059571, Boylam: 36.1478466\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "# Adres olarak tam metni belirtiyoruz\n",
    "address = \"hatay esenlik mahallesi\"\n",
    "\n",
    "# Adresi enlem ve boylama Ã§evir\n",
    "location = geolocator.geocode(address)\n",
    "\n",
    "if location:\n",
    "    print(f\"Adres: {address}\")\n",
    "    print(f\"Enlem: {location.latitude}, Boylam: {location.longitude}\")\n",
    "else:\n",
    "    print(f\"Adres bulunamadÄ±: {address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_ment_link</th>\n",
       "      <th>content_wo_punct</th>\n",
       "      <th>content_wo_removed_english</th>\n",
       "      <th>content_wo_normalize</th>\n",
       "      <th>content_wo_tokenize</th>\n",
       "      <th>content_wo_stop</th>\n",
       "      <th>content_wo_lemmatized</th>\n",
       "      <th>content_no_rare_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Orhan Åevik</td>\n",
       "      <td>orhan_sevik</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>@haluklevent\\n @oguzhanugur\\n  hepinizden alla...</td>\n",
       "      <td>\\n \\n  hepinizden allah razÄ± olsun iyiki varsÄ±...</td>\n",
       "      <td>hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±...</td>\n",
       "      <td>hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±z deprem</td>\n",
       "      <td>hepinizden allah razÄ± olsun iyi ki varsÄ±nÄ±z de...</td>\n",
       "      <td>['hepinizden', 'allah', 'razÄ±', 'olsun', 'iyi'...</td>\n",
       "      <td>['hepinizden', 'razÄ±', 'varsÄ±nÄ±z', 'deprem']</td>\n",
       "      <td>['hep', 'razÄ±', 'var', 'deprem']</td>\n",
       "      <td>['hep', 'razÄ±', 'var', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>nursena</td>\n",
       "      <td>nurssxx_</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem</td>\n",
       "      <td>allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem</td>\n",
       "      <td>allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver deprem</td>\n",
       "      <td>allahÄ±m nolur onlara dayanma gÃ¼cÃ¼ ver deprem</td>\n",
       "      <td>allahÄ±m ne olur onlara dayanma gÃ¼cÃ¼ ver deprem</td>\n",
       "      <td>['allahÄ±m', 'ne', 'olur', 'onlara', 'dayanma',...</td>\n",
       "      <td>['onlara', 'dayanma', 'gÃ¼cÃ¼', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']</td>\n",
       "      <td>['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Elif ÅEKER</td>\n",
       "      <td>55ellllllif</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>replying to \\n@haluklevent\\n @danlabilic\\n and...</td>\n",
       "      <td>replying to \\n\\n \\n and \\n_harun\\nadÄ±yamanda a...</td>\n",
       "      <td>replying to      and  harun adÄ±yamanda ali taÅŸ...</td>\n",
       "      <td>harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hic...</td>\n",
       "      <td>harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hicr...</td>\n",
       "      <td>['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...</td>\n",
       "      <td>['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...</td>\n",
       "      <td>['haru', 'adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle',...</td>\n",
       "      <td>['adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle', 'sokak'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>Yusuf</td>\n",
       "      <td>yusufaltuns</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n@showtv\\n #deprem</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n\\n #deprem</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun    deprem</td>\n",
       "      <td>tuÄŸba sÃ¶dekoÄŸlu kovulsun deprem</td>\n",
       "      <td>tuÄŸba sodekoÄŸlu kovulsun deprem</td>\n",
       "      <td>['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']</td>\n",
       "      <td>['tuÄŸba', 'sodekoÄŸlu', 'kov', 'deprem']</td>\n",
       "      <td>['deprem']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-03 19:38:25</td>\n",
       "      <td>ğÌˆğ³ğ ğ®Ìˆğ« ğ‘ğšğ§</td>\n",
       "      <td>dryghtn2</td>\n",
       "      <td>Feb 8, 2023</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...</td>\n",
       "      <td>arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...</td>\n",
       "      <td>arkadaÅŸÄ±mÄ±za ulaÅŸamÄ±yoruz kahramanmaraÅŸ elbist...</td>\n",
       "      <td>['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...</td>\n",
       "      <td>['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...</td>\n",
       "      <td>['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     username       handle   tweet_date  \\\n",
       "0  2024-11-03 19:38:25  Orhan Åevik  orhan_sevik  Feb 8, 2023   \n",
       "1  2024-11-03 19:38:25      nursena     nurssxx_  Feb 8, 2023   \n",
       "2  2024-11-03 19:38:25   Elif ÅEKER  55ellllllif  Feb 8, 2023   \n",
       "3  2024-11-03 19:38:25        Yusuf  yusufaltuns  Feb 8, 2023   \n",
       "4  2024-11-03 19:38:25  ğÌˆğ³ğ ğ®Ìˆğ« ğ‘ğšğ§     dryghtn2  Feb 8, 2023   \n",
       "\n",
       "                                             content  \\\n",
       "0  @haluklevent\\n @oguzhanugur\\n  hepinizden alla...   \n",
       "1  allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem   \n",
       "2  replying to \\n@haluklevent\\n @danlabilic\\n and...   \n",
       "3       tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n@showtv\\n #deprem   \n",
       "4  arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...   \n",
       "\n",
       "                                   content_ment_link  \\\n",
       "0  \\n \\n  hepinizden allah razÄ± olsun iyiki varsÄ±...   \n",
       "1  allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver #deprem   \n",
       "2  replying to \\n\\n \\n and \\n_harun\\nadÄ±yamanda a...   \n",
       "3              tuÄŸba sÃ¶dekoÄŸlu kovulsun \\n\\n #deprem   \n",
       "4  arkadaÅŸimiza ulaÅŸamiyoruz\\nkahramanmaraÅŸ elbis...   \n",
       "\n",
       "                                    content_wo_punct  \\\n",
       "0       hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±...   \n",
       "1   allahÄ±m nolur sen onlara dayanma gÃ¼cÃ¼ ver deprem   \n",
       "2  replying to      and  harun adÄ±yamanda ali taÅŸ...   \n",
       "3                 tuÄŸba sÃ¶dekoÄŸlu kovulsun    deprem   \n",
       "4  arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...   \n",
       "\n",
       "                          content_wo_removed_english  \\\n",
       "0  hepinizden allah razÄ± olsun iyiki varsÄ±nÄ±z deprem   \n",
       "1       allahÄ±m nolur onlara dayanma gÃ¼cÃ¼ ver deprem   \n",
       "2   harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hic...   \n",
       "3                    tuÄŸba sÃ¶dekoÄŸlu kovulsun deprem   \n",
       "4  arkadaÅŸimiza ulaÅŸamiyoruz kahramanmaraÅŸ elbist...   \n",
       "\n",
       "                                content_wo_normalize  \\\n",
       "0  hepinizden allah razÄ± olsun iyi ki varsÄ±nÄ±z de...   \n",
       "1     allahÄ±m ne olur onlara dayanma gÃ¼cÃ¼ ver deprem   \n",
       "2  harun adÄ±yamanda ali taÅŸÄ± mahallesi sokak hicr...   \n",
       "3                    tuÄŸba sodekoÄŸlu kovulsun deprem   \n",
       "4  arkadaÅŸÄ±mÄ±za ulaÅŸamÄ±yoruz kahramanmaraÅŸ elbist...   \n",
       "\n",
       "                                 content_wo_tokenize  \\\n",
       "0  ['hepinizden', 'allah', 'razÄ±', 'olsun', 'iyi'...   \n",
       "1  ['allahÄ±m', 'ne', 'olur', 'onlara', 'dayanma',...   \n",
       "2  ['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...   \n",
       "3       ['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']   \n",
       "4  ['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...   \n",
       "\n",
       "                                     content_wo_stop  \\\n",
       "0       ['hepinizden', 'razÄ±', 'varsÄ±nÄ±z', 'deprem']   \n",
       "1     ['onlara', 'dayanma', 'gÃ¼cÃ¼', 'ver', 'deprem']   \n",
       "2  ['harun', 'adÄ±yamanda', 'ali', 'taÅŸÄ±', 'mahall...   \n",
       "3       ['tuÄŸba', 'sodekoÄŸlu', 'kovulsun', 'deprem']   \n",
       "4  ['arkadaÅŸÄ±mÄ±za', 'ulaÅŸamÄ±yoruz', 'kahramanmara...   \n",
       "\n",
       "                               content_wo_lemmatized  \\\n",
       "0                   ['hep', 'razÄ±', 'var', 'deprem']   \n",
       "1             ['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']   \n",
       "2  ['haru', 'adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle',...   \n",
       "3            ['tuÄŸba', 'sodekoÄŸlu', 'kov', 'deprem']   \n",
       "4  ['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...   \n",
       "\n",
       "                               content_no_rare_words  \n",
       "0                   ['hep', 'razÄ±', 'var', 'deprem']  \n",
       "1             ['o', 'dayan', 'gÃ¼c', 'ver', 'deprem']  \n",
       "2  ['adÄ±yaman', 'ali', 'taÅŸÄ±', 'mahalle', 'sokak'...  \n",
       "3                                         ['deprem']  \n",
       "4  ['arkadaÅŸ', 'ulaÅŸ', 'kahramanmaraÅŸ', 'elbistan...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/TweetAnalyzeResult.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize(text):\n",
    "    words = text.split()\n",
    "    capitalized_words = [word.capitalize() for word in words]\n",
    "    capitalized_text = ' '.join(capitalized_words)\n",
    "    return capitalized_text\n",
    "\n",
    "df['content_wo_tokenize_capitalize'] = df['content_wo_normalize'].apply(capitalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_from_text(text):\n",
    "    if not text.strip():\n",
    "        return None \n",
    "    result = pipe(text) \n",
    "    location = \"\" \n",
    "    \n",
    "    for entity in result:\n",
    "        token = entity['word']\n",
    "        label = entity['entity']\n",
    "\n",
    "        if 'LOC' in label:  \n",
    "            if not token.startswith('##'):  \n",
    "                if location:  \n",
    "                    location += \" \" \n",
    "                location += token\n",
    "            else:\n",
    "                location += token.lstrip('##')  \n",
    "   \n",
    "\n",
    "    return location.strip() if location else None\n",
    "\n",
    "df['location'] = df['content_wo_tokenize_capitalize'].apply(get_location_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    unique_words = list(set(words))\n",
    "    unique_words.sort(key=lambda x: words.index(x))  \n",
    "    return ' '.join(unique_words)\n",
    "\n",
    "df['location_unique'] = df['location'].apply(remove_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_location'] = df['location_unique'].apply(lambda x: x.split() if pd.notnull(x) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "def get_coordinates(address):\n",
    "    if not address or pd.isna(address):\n",
    "        return None, None\n",
    "    try:\n",
    "        location = geolocator.geocode(address, timeout=10)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.388756, 35.916489)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coordinates(\"Hatay Arsuz gÃ¶zcÃ¼ler \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkiye_adres = pd.read_excel(\"../data/ililcemahalle.xlsx\")\n",
    "\n",
    "turkiye_adres[\"il\"] = turkiye_adres[\"il\"].str.strip()\n",
    "turkiye_adres[\"ilÃ§e\"] = turkiye_adres[\"ilÃ§e\"].str.strip()\n",
    "turkiye_adres[\"semt_bucak_belde\"] = turkiye_adres[\"semt_bucak_belde\"].str.strip()\n",
    "turkiye_adres[\"Mahalle\"] = turkiye_adres[\"Mahalle\"].str.strip()\n",
    "\n",
    "iller = set(turkiye_adres[\"il\"].str.lower().unique())\n",
    "ilceler = set(turkiye_adres[\"ilÃ§e\"].str.lower().unique())\n",
    "semtler = set(turkiye_adres[\"semt_bucak_belde\"].str.lower().unique())\n",
    "mahalleler = set(turkiye_adres[\"Mahalle\"].str.lower().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_in_set(token, keyword_set):\n",
    "    return [keyword for keyword in keyword_set if keyword in token]\n",
    "\n",
    "def clean_address(address):\n",
    "    return [word for word in address if 'hal' not in word.lower()]\n",
    "\n",
    "def build_address_from_tokens(tokens):\n",
    "    if not isinstance(tokens, list) or not tokens or len(tokens) == 1:\n",
    "        return None  \n",
    "    \n",
    "    found_parts = []  \n",
    "    tokens = [token.lower() for token in tokens]  \n",
    "\n",
    "    for token in tokens:\n",
    "        if (matched_parts := contains_in_set(token, iller)):\n",
    "            if matched_parts[0] not in found_parts:\n",
    "                found_parts.append(matched_parts[0])  \n",
    "        elif (matched_parts := contains_in_set(token, ilceler)):\n",
    "            if matched_parts[0] not in found_parts:\n",
    "                found_parts.append(matched_parts[0])\n",
    "\n",
    "    found_parts = clean_address(found_parts)\n",
    "    \n",
    "    return \" \".join(found_parts) if found_parts else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"address\"] = df[\"tokenized_location\"].apply(build_address_from_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coordinates(row):\n",
    "    if pd.notna(row['address']) and len(row['address'].split()) > 1: \n",
    "        return get_coordinates(row['address'])\n",
    "    return None, None  \n",
    "\n",
    "\n",
    "df[\"latitude\"], df[\"longitude\"] = zip(*df.apply(update_coordinates, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sehirler = [\n",
    "    \"Adana\", \"AdÄ±yaman\", \"Afyon\", \"AÄŸrÄ±\", \"Amasya\", \"Ankara\", \"Antalya\", \"Artvin\",\n",
    "    \"AydÄ±n\", \"BalÄ±kesir\", \"Bilecik\", \"BingÃ¶l\", \"Bitlis\", \"Bolu\", \"Burdur\", \"Bursa\", \"Ã‡anakkale\",\n",
    "    \"Ã‡ankÄ±rÄ±\", \"Ã‡orum\", \"Denizli\", \"DiyarbakÄ±r\", \"Edirne\", \"ElazÄ±ÄŸ\", \"Erzincan\", \"Erzurum\", \n",
    "    \"EskiÅŸehir\", \"Gaziantep\", \"Giresun\", \"GÃ¼mÃ¼ÅŸhane\", \"Hakkari\", \"Hatay\", \"Isparta\", \"Mersin\",\n",
    "    \"Ä°stanbul\", \"Ä°zmir\", \"Kars\", \"Kastamonu\", \"Kayseri\", \"KÄ±rklareli\", \"KÄ±rÅŸehir\", \"Kocaeli\",\n",
    "    \"Konya\", \"KÃ¼tahya\", \"Malatya\", \"Manisa\", \"KahramanmaraÅŸ\", \"Mardin\", \"MuÄŸla\", \"MuÅŸ\", \n",
    "    \"NevÅŸehir\", \"NiÄŸde\", \"Ordu\", \"Rize\", \"Sakarya\", \"Samsun\", \"Siirt\", \"Sinop\", \"Sivas\", \n",
    "    \"TekirdaÄŸ\", \"Tokat\", \"Trabzon\", \"Tunceli\", \"ÅanlÄ±urfa\", \"UÅŸak\", \"Van\", \"Yozgat\", \n",
    "    \"Zonguldak\", \"Aksaray\", \"Bayburt\", \"Karaman\", \"KÄ±rÄ±kkale\", \"Batman\", \"ÅÄ±rnak\", \n",
    "    \"BartÄ±n\", \"Ardahan\", \"IÄŸdÄ±r\", \"Yalova\", \"KarabÃ¼k\", \"Kilis\", \"Osmaniye\", \"DÃ¼zce\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cities(tokens, city_list):\n",
    "    if not tokens:  \n",
    "        return []\n",
    "    matches = []\n",
    "    for city in city_list:\n",
    "        for token in tokens:\n",
    "            if re.search(r'\\b' + re.escape(city) + r'\\b', token, re.IGNORECASE): \n",
    "                matches.append(city)\n",
    "    return list(set(matches))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'] = df['tokenized_location'].apply(lambda x: extract_cities(x, sehirler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get latitude & longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_city_coordinates(row):\n",
    "    if isinstance(row['city'], list) and len(row['city']) > 0 and pd.notna(row['city'][0]):\n",
    "        return get_coordinates(row['city'][0])\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"latitude_city\"], df[\"longitude_city\"] = zip(*df.apply(update_city_coordinates, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../results/LocationResult.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
